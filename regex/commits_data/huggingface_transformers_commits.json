{
  "repo_name": "huggingface/transformers",
  "commits": [
    {
      "sha": "dcbdf7e962c4b36140cc9ee76f870016121e69e5",
      "message": "Fix _load_state_dict_into_meta_model with device_map=None (#36488)\n\n* Fix _load_state_dict_into_meta_model with device_map=None\n\n* Update src/transformers/modeling_utils.py",
      "changes": [
        {
          "file": "src/transformers/modeling_utils.py",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -785,8 +785,8 @@ def _load_state_dict_into_meta_model(\n     tensor_device = None\n     if device_map is not None and device_map.get(\"\", None) is not None:\n         tensor_device = device_map[\"\"].index if isinstance(device_map[\"\"], torch.device) else device_map[\"\"]\n-\n-    device_map_regex = \"|\".join(sorted(device_map.keys(), reverse=True))\n+    if device_map is not None:\n+        device_map_regex = \"|\".join(sorted(device_map.keys(), reverse=True))\n \n     # we need this later to initialize tensor parallelism\n     if device_mesh is not None:"
        }
      ]
    },
    {
      "sha": "2c5d038f9204ffbc78acd398238dd5231341e648",
      "message": "Add Got-OCR 2 Fast image processor and refactor slow one (#36185)\n\n* refactor image processor slow got ocr\n\n* add working image processor fast\n\n* fix fast image processor, update doc\n\n* use one big loop for processing patches",
      "changes": [
        {
          "file": "src/transformers/__init__.py",
          "additions": 2,
          "deletions": 0,
          "patch": "@@ -1330,6 +1330,7 @@\n     _import_structure[\"models.deit\"].append(\"DeiTImageProcessorFast\")\n     _import_structure[\"models.depth_pro\"].append(\"DepthProImageProcessorFast\")\n     _import_structure[\"models.detr\"].append(\"DetrImageProcessorFast\")\n+    _import_structure[\"models.got_ocr2\"].append(\"GotOcr2ImageProcessorFast\")\n     _import_structure[\"models.llava\"].append(\"LlavaImageProcessorFast\")\n     _import_structure[\"models.llava_next\"].append(\"LlavaNextImageProcessorFast\")\n     _import_structure[\"models.llava_onevision\"].append(\"LlavaOnevisionImageProcessorFast\")\n@@ -6526,6 +6527,7 @@\n         from .models.deit import DeiTImageProcessorFast\n         from .models.depth_pro import DepthProImageProcessorFast\n         from .models.detr import DetrImageProcessorFast\n+        from .models.got_ocr2 import GotOcr2ImageProcessorFast\n         from .models.llava import LlavaImageProcessorFast\n         from .models.llava_next import LlavaNextImageProcessorFast\n         from .models.llava_onevision import LlavaOnevisionImageProcessorFast"
        },
        {
          "file": "src/transformers/models/auto/image_processing_auto.py",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -88,7 +88,7 @@\n             (\"fuyu\", (\"FuyuImageProcessor\",)),\n             (\"git\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"glpn\", (\"GLPNImageProcessor\",)),\n-            (\"got_ocr2\", (\"GotOcr2ImageProcessor\",)),\n+            (\"got_ocr2\", (\"GotOcr2ImageProcessor\", \"GotOcr2ImageProcessorFast\")),\n             (\"grounding-dino\", (\"GroundingDinoImageProcessor\",)),\n             (\"groupvit\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"hiera\", (\"BitImageProcessor\",)),"
        },
        {
          "file": "src/transformers/models/got_ocr2/__init__.py",
          "additions": 1,
          "deletions": 0,
          "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_got_ocr2 import *\n     from .image_processing_got_ocr2 import *\n+    from .image_processing_got_ocr2_fast import *\n     from .modeling_got_ocr2 import *\n     from .processing_got_ocr2 import *\n "
        },
        {
          "file": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
          "additions": 1,
          "deletions": 5,
          "patch": "@@ -32,11 +32,7 @@\n from ...generation import GenerationMixin\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    replace_return_docstrings,\n-)\n+from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings\n from ..auto import AutoModelForCausalLM\n from .configuration_got_ocr2 import GotOcr2Config, GotOcr2VisionConfig\n "
        },
        {
          "file": "src/transformers/utils/dummy_torchvision_objects.py",
          "additions": 7,
          "deletions": 0,
          "patch": "@@ -58,6 +58,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torchvision\"])\n \n \n+class GotOcr2ImageProcessorFast(metaclass=DummyObject):\n+    _backends = [\"torchvision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torchvision\"])\n+\n+\n class LlavaImageProcessorFast(metaclass=DummyObject):\n     _backends = [\"torchvision\"]\n "
        }
      ]
    },
    {
      "sha": "02776d2c6aa997c5b81f28f2edf38df9967253be",
      "message": "Fix loading models with mismatched sizes (#36463)\n\n* Fix loading model with mismatched sizes\n\n* trigger tests",
      "changes": [
        {
          "file": "src/transformers/modeling_utils.py",
          "additions": 3,
          "deletions": 1,
          "patch": "@@ -4907,7 +4907,9 @@ def _load_pretrained_model(\n                     model_to_load, state_dict, start_prefix\n                 )\n                 # at this point the state dict should be on cpu, we don't need to actually read it\n-                fixed_state_dict = model_to_load._fix_state_dict_keys_on_load(state_dict)\n+                mismatched_names = [name for name, _, _ in mismatched_keys]\n+                fixed_state_dict = {k: v for k, v in state_dict.items() if k not in mismatched_names}\n+                fixed_state_dict = model_to_load._fix_state_dict_keys_on_load(fixed_state_dict)\n                 model_to_load.load_state_dict(fixed_state_dict, strict=False, assign=assign_to_params_buffers)\n         else:\n             # This should always be a list but, just to be sure."
        }
      ]
    },
    {
      "sha": "222505c7e4d08da9095d12ddb72fb653f4b6da33",
      "message": "[GroundingDino] Fix grounding dino loss \ud83d\udea8 (#31828)\n\n* Starting to fix GroundingDinoLoss and GroundingDinoHungarianMatcher\n\n* More updates\n\n* More updates\n\n* fixed: GroundingDinoLoss\n\n* fixed: failing tests\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update tests/models/grounding_dino/test_modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Addressed comments\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: Sangbum Daniel Choi <34004152+SangbumChoi@users.noreply.github.com>\n\n* add: cardinality loss and make box loss as copy from\n\n* change: default for reduction loss is sum\n\n* fix: vectorized generate fake box\n\n* fix copies\n\n* Addressed comments\n\n* addressed comments\n\n* addressed one-hot\n\n* Update tests/models/grounding_dino/test_modeling_grounding_dino.py\n\nCo-authored-by: Sangbum Daniel Choi <34004152+SangbumChoi@users.noreply.github.com>\n\n* Addressed comments\n\n* fixed test\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\n* Update tests/models/grounding_dino/test_modeling_grounding_dino.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Starting to fix GroundingDinoLoss and GroundingDinoHungarianMatcher\n\n* More updates\n\n* More updates\n\n* fixed: GroundingDinoLoss\n\n* add: cardinality loss and make box loss as copy from\n\n* fix copies\n\n* Revert \"Update tests/models/grounding_dino/test_modeling_grounding_dino.py\"\n\nThis reverts commit aa74c4c57c430e54cc74c414d6269edb65c73e83.\n\n* [run-slow] groundigdino\n\n* remove nestedtensor\n\n* [run-slow] groundig_dino\n\n* [run-slow] grounding_dino\n\n* [run-slow] grounding_dino\n\n* [run-slow] grounding_dino\n\n* check\n\n* check\n\n* add: enconder intermediate outputs to ImageLoss forward\n\n* add: GroundingDinoForObjectDetectionLoss in the loss directory\n\n* make style\n\n* fix the loss function\n\n* remove class_reduction since it sum is default\n\n* remove class_reduction\n\n* Update src/transformers/loss/loss_grounding_dino.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* simple fix\n\n* Update src/transformers/loss/loss_grounding_dino.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* minor fix\n\n* Update src/transformers/loss/loss_for_object_detection.py\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\nCo-authored-by: Sangbum Daniel Choi <34004152+SangbumChoi@users.noreply.github.com>\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\nCo-authored-by: sangbumchoi <danielsejong55@gmail.com>\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
      "changes": [
        {
          "file": "src/transformers/loss/loss_utils.py",
          "additions": 2,
          "deletions": 1,
          "patch": "@@ -18,6 +18,7 @@\n \n from .loss_deformable_detr import DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n from .loss_for_object_detection import ForObjectDetectionLoss, ForSegmentationLoss\n+from .loss_grounding_dino import GroundingDinoForObjectDetectionLoss\n from .loss_rt_detr import RTDetrForObjectDetectionLoss\n \n \n@@ -129,7 +130,7 @@ def ForTokenClassification(logits, labels, config, **kwargs):\n     \"DeformableDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n     \"ConditionalDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n     \"DabDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n-    \"GroundingDinoForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n+    \"GroundingDinoForObjectDetection\": GroundingDinoForObjectDetectionLoss,\n     \"ConditionalDetrForSegmentation\": DeformableDetrForSegmentationLoss,\n     \"RTDetrForObjectDetection\": RTDetrForObjectDetectionLoss,\n     \"RTDetrV2ForObjectDetection\": RTDetrForObjectDetectionLoss,"
        }
      ]
    },
    {
      "sha": "482d17be60d1836a7e787c5c3d69d4c3ed171ebc",
      "message": "Fix `hub_retry` (#36449)\n\n* cry\n\n* trigger\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
      "changes": [
        {
          "file": "tests/test_modeling_common.py",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -223,7 +223,7 @@ def __init_subclass__(cls, **kwargs):\n             if attr_name.startswith(\"test_\"):\n                 attr = getattr(cls, attr_name)\n                 if callable(attr):\n-                    setattr(cls, attr_name, hub_retry(attr))\n+                    setattr(cls, attr_name, hub_retry()(attr))\n \n     @property\n     def all_generative_model_classes(self):"
        }
      ]
    },
    {
      "sha": "a7fbab33aeb865010f752e3840304f4b2cddb31b",
      "message": "Fix Expected output for compressed-tensors tests (#36425)\n\nfix",
      "changes": [
        {
          "file": "tests/quantization/compressed_tensors/test_compressed_tensors.py",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -47,7 +47,7 @@ def test_config_to_from_dict(self):\n         self.assertIsInstance(config_from_dict.sparsity_config, SparsityCompressionConfig)\n \n     def test_tinyllama_w8a8(self):\n-        expected_out = \"<s> Paris is the capital of which country?\\n\\n**A) Paris**\\n\\n**Q** ** Paris is the capital of which country?\\n\\n**A) Paris**\\n\\n**Q** ** Paris is the capital of which country\"\n+        expected_out = \"<s> Paris is the capital of which country?\\n\\n  1. Paris is the capital of which country?\\n\\n  1. Paris is the capital of which country?\\n\\n  1. Paris is the capital of which country?\\n\\n\"\n         self._test_quantized_model(self.tinyllama_w8a8, expected_out)\n \n     def test_tinyllama_w4a16(self):\n@@ -59,7 +59,7 @@ def test_tinyllama_w8a16(self):\n         self._test_quantized_model(self.tinyllama_w8a16, expected_out)\n \n     def test_llama_8b_fp8(self):\n-        expected_out = \"<|begin_of_text|>Paris is the capital of which country? France\\nWhat is the name of the famous art museum in Paris? The Louvre\\nWhat is the name of the famous opera house in Paris? Palais Garnier\\nWhat is the name of the\"\n+        expected_out = \"<|begin_of_text|>Paris is the capital of which country? France\\nWhat is the name of the famous museum in Paris that is home to the Mona Lisa? The Louvre\\nWhat is the name of the famous bridge in Paris that is often associated with the city\"\n         self._test_quantized_model(self.llama3_8b_fp8, expected_out)\n \n     def _test_quantized_model(self, model_name: str, expected_output: str):"
        }
      ]
    },
    {
      "sha": "082834dd79d14a2e53332c0c1a7c19852ab8973f",
      "message": "fix: prevent model access error during Optuna hyperparameter tuning (#36395)\n\n* fix: prevent model access error during Optuna hyperparameter tuning\n\nThe `transformers.integrations.integration_utils.run_hp_search_optuna` function releases model memory and sets trainer.model to None after each trial. This causes an AttributeError when  subsequent Trainer.train calls attempt to access the model before reinitialization. This is only an issue when `fp16_full_eval` or `bf16_full_eval` flags are enabled.\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
      "changes": [
        {
          "file": "src/transformers/trainer.py",
          "additions": 6,
          "deletions": 1,
          "patch": "@@ -2180,7 +2180,12 @@ def train(\n \n         # do_train is not a reliable argument, as it might not be set and .train() still called, so\n         # the following is a workaround:\n-        if (args.fp16_full_eval or args.bf16_full_eval) and not args.do_train and not self.is_model_parallel:\n+        if (\n+            (args.fp16_full_eval or args.bf16_full_eval)\n+            and not args.do_train\n+            and not self.is_model_parallel\n+            and self.model_init is None\n+        ):\n             self._move_model_to_device(self.model, args.device)\n \n         if \"model_path\" in kwargs:"
        }
      ]
    },
    {
      "sha": "9ebfda3263fcdc4e05fd87fad1aadc8a08294608",
      "message": "Fixed VitDet for non-squre Images (#35969)\n\n* size tuple\n\n* delete original input_size\n\n* use zip\n\n* process the other case\n\n* Update src/transformers/models/vitdet/modeling_vitdet.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* [VITDET] Test non-square image\n\n* [Fix] Make Quality\n\n* make fix style\n\n* Update src/transformers/models/vitdet/modeling_vitdet.py\n\n---------\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
      "changes": [
        {
          "file": "src/transformers/models/vitdet/modeling_vitdet.py",
          "additions": 7,
          "deletions": 1,
          "patch": "@@ -456,8 +456,14 @@ def __init__(\n         super().__init__()\n \n         dim = config.hidden_size\n-        input_size = (config.image_size // config.patch_size, config.image_size // config.patch_size)\n \n+        image_size = config.image_size\n+        image_size = image_size if isinstance(image_size, (list, tuple)) else (image_size, image_size)\n+\n+        patch_size = config.patch_size\n+        patch_size = patch_size if isinstance(patch_size, (list, tuple)) else (patch_size, patch_size)\n+\n+        input_size = (image_size[0] // patch_size[0], image_size[1] // patch_size[1])\n         self.norm1 = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n         self.attention = VitDetAttention(\n             config, input_size=input_size if window_size == 0 else (window_size, window_size)"
        }
      ]
    },
    {
      "sha": "88d10517b4c2c1aceeee89787db2578653667beb",
      "message": "Fix convert_to_rgb for SAM ImageProcessor (#36369)",
      "changes": [
        {
          "file": "src/transformers/models/sam/image_processing_sam.py",
          "additions": 0,
          "deletions": 2,
          "patch": "@@ -307,8 +307,6 @@ def _preprocess_image(\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> Tuple[np.ndarray, Tuple[int, int], Tuple[int, int]]:\n-        image = to_numpy_array(image)\n-\n         # PIL RGBA images are converted to RGB\n         if do_convert_rgb:\n             image = convert_to_rgb(image)"
        }
      ]
    },
    {
      "sha": "ca6ebcb9bca050796bb53be37a9bad68f0f03b3f",
      "message": "chore: fix function argument descriptions (#36392)",
      "changes": [
        {
          "file": "src/transformers/commands/add_new_model_like.py",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -1523,7 +1523,7 @@ def get_user_field(\n         is_valid_answer (`Callable`, *optional*):\n             If set, the question will be asked until this function returns `True` on the provided answer.\n         convert_to (`Callable`, *optional*):\n-            If set, the answer will be passed to this function. If this function raises an error on the procided\n+            If set, the answer will be passed to this function. If this function raises an error on the provided\n             answer, the question will be asked again.\n         fallback_message (`str`, *optional*):\n             A message that will be displayed each time the question is asked again to the user.\n@@ -1710,7 +1710,7 @@ def get_user_input():\n         frameworks = None\n     else:\n         frameworks = get_user_field(\n-            \"Please enter the list of framworks you want (pt, tf, flax) separated by spaces\",\n+            \"Please enter the list of frameworks you want (pt, tf, flax) separated by spaces\",\n             is_valid_answer=lambda x: all(p in [\"pt\", \"tf\", \"flax\"] for p in x.split(\" \")),\n         )\n         frameworks = list(set(frameworks.split(\" \")))"
        }
      ]
    },
    {
      "sha": "b4b9da6d9b3fef860889bb843bcf10e8d5e2646a",
      "message": "tests: revert change of torch_require_multi_gpu to be device agnostic (#35721)\n\n* tests: revert change of torch_require_multi_gpu to be device agnostic\n\nThe 11c27dd33 modified `torch_require_multi_gpu()` to be device agnostic\ninstead of being CUDA specific. This broke some tests which are rightfully\nCUDA specific, such as:\n\n* `tests/trainer/test_trainer_distributed.py::TestTrainerDistributed`\n\nIn the current Transformers tests architecture `require_torch_multi_accelerator()`\nshould be used to mark multi-GPU tests agnostic to device.\n\nThis change addresses the issue introduced by 11c27dd33 and reverts\nmodification of `torch_require_multi_gpu()`.\n\nFixes: 11c27dd33 (\"Enable BNB multi-backend support (#31098)\")\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>\n\n* fix bug: modification of frozen set\n\n---------\n\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>\nCo-authored-by: Titus von Koeller <9048635+Titus-von-Koeller@users.noreply.github.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
      "changes": [
        {
          "file": "src/transformers/integrations/bitsandbytes.py",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -486,7 +486,7 @@ def _validate_bnb_multi_backend_availability(raise_exception):\n     import bitsandbytes as bnb\n \n     bnb_supported_devices = getattr(bnb, \"supported_torch_devices\", set())\n-    available_devices = get_available_devices()\n+    available_devices = set(get_available_devices())\n \n     if available_devices == {\"cpu\"} and not is_ipex_available():\n         from importlib.util import find_spec"
        },
        {
          "file": "tests/quantization/bnb/test_4bit.py",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -39,7 +39,7 @@\n     require_bitsandbytes,\n     require_torch,\n     require_torch_gpu_if_bnb_not_multi_backend_enabled,\n-    require_torch_multi_gpu,\n+    require_torch_multi_accelerator,\n     slow,\n     torch_device,\n )\n@@ -517,7 +517,7 @@ def test_pipeline(self):\n         self.assertIn(pipeline_output[0][\"generated_text\"], self.EXPECTED_OUTPUTS)\n \n \n-@require_torch_multi_gpu\n+@require_torch_multi_accelerator\n @apply_skip_if_not_implemented\n class Bnb4bitTestMultiGpu(Base4bitTest):\n     def setUp(self):"
        },
        {
          "file": "tests/quantization/bnb/test_mixed_int8.py",
          "additions": 3,
          "deletions": 3,
          "patch": "@@ -39,7 +39,7 @@\n     require_bitsandbytes,\n     require_torch,\n     require_torch_gpu_if_bnb_not_multi_backend_enabled,\n-    require_torch_multi_gpu,\n+    require_torch_multi_accelerator,\n     slow,\n     torch_device,\n )\n@@ -671,7 +671,7 @@ def test_pipeline(self):\n         self.assertIn(pipeline_output[0][\"generated_text\"], self.EXPECTED_OUTPUTS)\n \n \n-@require_torch_multi_gpu\n+@require_torch_multi_accelerator\n @apply_skip_if_not_implemented\n class MixedInt8TestMultiGpu(BaseMixedInt8Test):\n     def setUp(self):\n@@ -700,7 +700,7 @@ def test_multi_gpu_loading(self):\n         self.assertIn(self.tokenizer.decode(output_parallel[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n \n-@require_torch_multi_gpu\n+@require_torch_multi_accelerator\n @apply_skip_if_not_implemented\n class MixedInt8TestCpuGpu(BaseMixedInt8Test):\n     def setUp(self):"
        }
      ]
    },
    {
      "sha": "d80d52b007273af8049541e15441e59551f129ce",
      "message": "addressing the issue #34611 to make FlaxDinov2 compatible with any batch size (#35138)\n\nfixed the batch_size error, all tests are passing\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
      "changes": [
        {
          "file": "src/transformers/models/dinov2/modeling_flax_dinov2.py",
          "additions": 5,
          "deletions": 3,
          "patch": "@@ -185,9 +185,11 @@ def interpolate_pos_encoding(self, config, hidden_states, height, width, positio\n             antialias=False,\n         )\n         patch_pos_embed = patch_pos_embed.astype(target_dtype)\n-        patch_pos_embed = jnp.transpose(patch_pos_embed, (0, 2, 3, 1)).reshape((hidden_states.shape[0], -1, dim))\n+        patch_pos_embed = jnp.transpose(patch_pos_embed, (0, 2, 3, 1)).reshape((position_embeddings.shape[0], -1, dim))\n+        patch_pos_embed_expanded = jnp.tile(patch_pos_embed, (hidden_states.shape[0], 1, 1))\n+        class_pos_embed_expanded = jnp.tile(class_pos_embed, (hidden_states.shape[0], 1, 1))\n \n-        return jnp.concatenate((class_pos_embed[jnp.newaxis, :], patch_pos_embed), axis=1)\n+        return jnp.concatenate((class_pos_embed_expanded, patch_pos_embed_expanded), axis=1)\n \n     def __call__(self, pixel_values, deterministic=True):\n         batch_size = pixel_values.shape[0]\n@@ -778,7 +780,7 @@ class FlaxDinov2ForImageClassification(FlaxDinov2PreTrainedModel):\n     >>> image = Image.open(requests.get(url, stream=True).raw)\n \n     >>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base-imagenet1k-1-layer\")\n-    >>> model = FlaxDinov2ForImageClassification.from_pretrained(\"facebook/dinov2-base-imagenet1k-1-layer\")\n+    >>> model = FlaxDinov2ForImageClassification.from_pretrained(\"facebook/dinov2-base-imagenet1k-1-layer\", from_pt=True)\n \n     >>> inputs = image_processor(images=image, return_tensors=\"np\")\n     >>> outputs = model(**inputs)"
        }
      ]
    },
    {
      "sha": "da4ab2a1b66e2367f94ea34438d344dd53e2d66e",
      "message": "Fix doc formatting in forward passes & modular (#36243)\n\n* fix indentation issues + modular without magic keyword\n\n* style\n\n* Update doc.py\n\n* style\n\n* Fix all decorators indentation\n\n* all models\n\n* style\n\n* style\n\n* Update doc.py\n\n* fix\n\n* general fix\n\n* style",
      "changes": [
        {
          "file": "examples/modular-transformers/modeling_new_task_model.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -349,7 +349,6 @@ def forward(\n         num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, NewTaskModelCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/aria/modeling_aria.py",
          "additions": 0,
          "deletions": 2,
          "patch": "@@ -1193,7 +1193,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1458,7 +1457,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `Idefics3ForConditionalGeneration`)."
        },
        {
          "file": "src/transformers/models/aria/modular_aria.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1437,7 +1437,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `Idefics3ForConditionalGeneration`)."
        },
        {
          "file": "src/transformers/models/bamba/modeling_bamba.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1495,7 +1495,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/bamba/modular_bamba.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1205,7 +1205,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/chameleon/modeling_chameleon.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1554,7 +1554,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/cohere/modeling_cohere.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -833,7 +833,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/cohere/modular_cohere.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -321,7 +321,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/cohere2/modeling_cohere2.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -834,7 +834,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/dbrx/modeling_dbrx.py",
          "additions": 1,
          "deletions": 3,
          "patch": "@@ -1283,9 +1283,7 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n-        r\"\"\"Forward function for causal language modeling.\n-\n-        Args:\n+        r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -716,7 +716,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/diffllama/modeling_diffllama.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1070,7 +1070,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/emu3/modeling_emu3.py",
          "additions": 0,
          "deletions": 2,
          "patch": "@@ -1650,7 +1650,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1878,7 +1877,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/emu3/modular_emu3.py",
          "additions": 0,
          "deletions": 2,
          "patch": "@@ -1077,7 +1077,6 @@ def __init__(self, config):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"Emu3TextConfig\")\n     def forward(**super_kwargs):\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -1186,7 +1185,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/gemma/modeling_gemma.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -803,7 +803,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/gemma/modular_gemma.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -483,7 +483,6 @@ def forward(\n class GemmaForCausalLM(LlamaForCausalLM):\n     def forward(**super_kwargs):\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/gemma2/modeling_gemma2.py",
          "additions": 2,
          "deletions": 3,
          "patch": "@@ -841,7 +841,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -859,9 +858,9 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from transformers import AutoTokenizer, GemmaForCausalLM\n+        >>> from transformers import AutoTokenizer, Gemma2ForCausalLM\n \n-        >>> model = GemmaForCausalLM.from_pretrained(\"google/gemma-2-9b\")\n+        >>> model = Gemma2ForCausalLM.from_pretrained(\"google/gemma-2-9b\")\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\n \n         >>> prompt = \"What is your favorite condiment?\""
        },
        {
          "file": "src/transformers/models/glm/modeling_glm.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -812,7 +812,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -769,7 +769,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, GotOcr2CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -848,7 +848,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, LlavaCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/granite/modeling_granite.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -815,7 +815,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/granitemoe/modeling_granitemoe.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1287,7 +1287,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1313,7 +1313,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/helium/modeling_helium.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -799,7 +799,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/idefics/modeling_idefics.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1559,7 +1559,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, IdeficsCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/idefics/modeling_tf_idefics.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1687,7 +1687,6 @@ def call(\n         training=False,\n     ) -> Union[TFIdeficsCausalLMOutputWithPast, Tuple[tf.Tensor]]:\n         r\"\"\"\n-        Args:\n             labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/idefics2/modeling_idefics2.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1537,7 +1537,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, Idefics2CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `Idefics2ForConditionalGeneration`)."
        },
        {
          "file": "src/transformers/models/idefics3/modeling_idefics3.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1121,7 +1121,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, Idefics3CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `Idefics3ForConditionalGeneration`)."
        },
        {
          "file": "src/transformers/models/jamba/modeling_jamba.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1456,7 +1456,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/jetmoe/modeling_jetmoe.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1299,7 +1299,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/llama/modeling_llama.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -801,7 +801,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/llava/modeling_llava.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -348,7 +348,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, LlavaCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/llava_next/modeling_llava_next.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -561,7 +561,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, LlavaNextCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -601,7 +601,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, LlavaNextVideoCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, image_size, image_size)):\n                 The tensors corresponding to the input videos. Pixel values can be obtained using\n                 [`AutoImageProcessor`]. See [`LlavaNextVideoVideoProcessor.__call__`] for details. [`LlavaProcessor`] uses"
        },
        {
          "file": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -360,7 +360,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, LlavaNextVideoCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, image_size, image_size)):\n                 The tensors corresponding to the input videos. Pixel values can be obtained using\n                 [`AutoImageProcessor`]. See [`LlavaNextVideoVideoProcessor.__call__`] for details. [`LlavaProcessor`] uses"
        },
        {
          "file": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -623,7 +623,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, LlavaOnevisionCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/mistral/modeling_mistral.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -802,7 +802,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/mixtral/modeling_mixtral.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1022,7 +1022,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/mixtral/modular_mixtral.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -480,7 +480,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/mllama/modeling_mllama.py",
          "additions": 0,
          "deletions": 2,
          "patch": "@@ -1901,7 +1901,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -2048,7 +2047,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/moshi/modeling_moshi.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1813,7 +1813,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, MoshiCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/nemotron/modeling_nemotron.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1047,7 +1047,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/olmo/modeling_olmo.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -777,7 +777,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/olmo2/modeling_olmo2.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -778,7 +778,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/olmoe/modeling_olmoe.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1206,7 +1206,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/paligemma/modeling_paligemma.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -438,7 +438,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, PaliGemmaCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/persimmon/modeling_persimmon.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -852,7 +852,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/phi/modeling_phi.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -775,7 +775,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/phi3/modeling_phi3.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -877,7 +877,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/phimoe/modeling_phimoe.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1388,7 +1388,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/qwen2/modeling_qwen2.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -815,7 +815,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1742,7 +1742,6 @@ def forward(\n         second_per_grid_ts: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -608,7 +608,6 @@ def forward(\n         second_per_grid_ts: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1112,7 +1112,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Qwen2AudioCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1272,7 +1272,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1619,7 +1619,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[Tuple, Qwen2VLCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -821,7 +821,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutput]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/stablelm/modeling_stablelm.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1109,7 +1109,6 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/starcoder2/modeling_starcoder2.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -798,7 +798,6 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/video_llava/modeling_video_llava.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -383,7 +383,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, VideoLlavaCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/vipllava/modeling_vipllava.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -323,7 +323,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[Tuple, VipLlavaCausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/zamba/modeling_zamba.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1228,7 +1228,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
          "file": "src/transformers/models/zamba2/modeling_zamba2.py",
          "additions": 0,
          "deletions": 1,
          "patch": "@@ -1665,7 +1665,6 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        }
      ]
    },
    {
      "sha": "92abc0dae848d1853d04b9e8b4672da2374d3cd4",
      "message": "Update _get_eval_sampler to reflect Trainer.tokenizer is deprecation  self.tokenizer -> self.processing_class (#36315)\n\n* fix warning self.tokenizer -> self.processing_class\n\n* formating change",
      "changes": [
        {
          "file": "src/transformers/trainer.py",
          "additions": 3,
          "deletions": 1,
          "patch": "@@ -1060,7 +1060,9 @@ def _get_eval_sampler(self, eval_dataset: Dataset) -> Optional[torch.utils.data.\n                 )\n             else:\n                 lengths = None\n-            model_input_name = self.tokenizer.model_input_names[0] if self.tokenizer is not None else None\n+            model_input_name = (\n+                self.processing_class.model_input_names[0] if self.processing_class is not None else None\n+            )\n             return LengthGroupedSampler(\n                 self.args.eval_batch_size,\n                 dataset=eval_dataset,"
        }
      ]
    },
    {
      "sha": "401543a825ca6e632cf53924a1cbcf82f44939e5",
      "message": "Fix `is_causal` fail with compile (#36374)\n\nfix",
      "changes": [
        {
          "file": "src/transformers/integrations/sdpa_attention.py",
          "additions": 2,
          "deletions": 1,
          "patch": "@@ -42,8 +42,9 @@ def sdpa_attention_forward(\n \n     # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n     # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+    # Note that it is important to check first for the shape, otherwise compile will fail with `argument 'is_causal' must be bool, not SymBool`\n     if is_causal is None:\n-        is_causal = causal_mask is None and query.shape[2] > 1\n+        is_causal = query.shape[2] > 1 and causal_mask is None\n \n     # Shapes (e.g. query.shape[2]) are tensors during jit tracing, resulting in `is_causal` being a tensor.\n     # We convert it to a bool for the SDPA kernel that only accepts bools."
        }
      ]
    },
    {
      "sha": "931e5f4ac375362be2e5020d54dec4d73dcc8321",
      "message": "Update modeling_llava_onevision.py (#36391)\n\nFixed a potential bug in modeling_llava_onevision.py",
      "changes": [
        {
          "file": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -740,7 +740,7 @@ def forward(\n \n             special_video_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n             special_video_mask = special_video_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_video_mask].numel() != video_features.numel():\n                 n_video_tokens = (input_ids == self.config.video_token_index).sum()\n                 n_video_features = video_features.shape[0]\n                 raise ValueError("
        }
      ]
    },
    {
      "sha": "18276b03f759771779c2995937ecaefa782a2c3c",
      "message": "fix(type): padding_side type should be Optional[str] (#36326)",
      "changes": [
        {
          "file": "src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py",
          "additions": 4,
          "deletions": 4,
          "patch": "@@ -277,7 +277,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -445,7 +445,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -600,7 +600,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[bool] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -662,7 +662,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\""
        },
        {
          "file": "src/transformers/models/led/tokenization_led.py",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -412,7 +412,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         encoded_inputs = super()._pad("
        },
        {
          "file": "src/transformers/models/led/tokenization_led_fast.py",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -280,7 +280,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         encoded_inputs = super()._pad("
        },
        {
          "file": "src/transformers/models/roc_bert/tokenization_roc_bert.py",
          "additions": 5,
          "deletions": 5,
          "patch": "@@ -210,7 +210,7 @@ def _encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -310,7 +310,7 @@ def prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -484,7 +484,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         # Load from model defaults\n@@ -557,7 +557,7 @@ def _batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -658,7 +658,7 @@ def _batch_prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,"
        },
        {
          "file": "src/transformers/models/wav2vec2/tokenization_wav2vec2.py",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -781,7 +781,7 @@ def __call__(\n         padding: Union[bool, str, PaddingStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         verbose: bool = True,\n         **kwargs,"
        },
        {
          "file": "src/transformers/tokenization_utils.py",
          "additions": 3,
          "deletions": 3,
          "patch": "@@ -752,7 +752,7 @@ def _encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -838,7 +838,7 @@ def _batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -920,7 +920,7 @@ def _batch_prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,"
        },
        {
          "file": "src/transformers/tokenization_utils_fast.py",
          "additions": 3,
          "deletions": 3,
          "patch": "@@ -427,7 +427,7 @@ def set_truncation_and_padding(\n         max_length: int,\n         stride: int,\n         pad_to_multiple_of: Optional[int],\n-        padding_side: Optional[bool],\n+        padding_side: Optional[str],\n     ):\n         \"\"\"\n         Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers\n@@ -507,7 +507,7 @@ def _batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -597,7 +597,7 @@ def _encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[bool] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,"
        }
      ]
    },
    {
      "sha": "884a8ea1f058716c24adb52a0a6a0bf41fbb973d",
      "message": "Improve model loading for compressed tensor models (#36152)\n\n* Disable warnings for stacked compressors\n* Introduce two new hooks in HfQuantizer lifecycle\nto allow updates to missing and unexpected keys\n* Update missing and unexpected keys\nfor stacked compressors\n* Add tests\n* Fix: run_compressed cases\n* Fix: uncompressed cases\n\n* Rename compressed_tensor folder to compressed_tensors\nMove RunCompressedTest to the same file\nUpdate tests to unittest",
      "changes": [
        {
          "file": "src/transformers/modeling_utils.py",
          "additions": 4,
          "deletions": 0,
          "patch": "@@ -4673,6 +4673,7 @@ def _load_pretrained_model(\n                 unexpected_keys = [k for k in unexpected_keys if re.search(pat, k) is None]\n         if hf_quantizer is not None:\n             missing_keys = hf_quantizer.update_missing_keys(model, missing_keys, prefix)\n+            unexpected_keys = hf_quantizer.update_unexpected_keys(model, unexpected_keys, prefix)\n \n         # retrieve weights on meta device and put them back on CPU.\n         # This is not ideal in terms of memory, but if we don't do that not, we can't initialize them in the next step\n@@ -4993,6 +4994,9 @@ def _find_mismatched_keys(\n                 load_offloaded_weights(model_to_load, state_dict_index, state_dict_folder)\n                 shutil.rmtree(state_dict_folder)\n \n+        if hf_quantizer is not None:\n+            missing_keys = hf_quantizer.update_missing_keys_after_loading(model_to_load, missing_keys, prefix)\n+\n         if len(error_msgs) > 0:\n             error_msg = \"\\n\\t\".join(error_msgs)\n             if \"size mismatch\" in error_msg:"
        }
      ]
    },
    {
      "sha": "4dbf17c17f5834eb68f296457acc605a8c533b5a",
      "message": "[tests] enable bnb tests on xpu (#36233)\n\n* fix failed test\n\n* fix device\n\n* fix more device cases\n\n* add more cases\n\n* fix empty cache\n\n* Update test_4bit.py\n\n---------\n\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
      "changes": [
        {
          "file": "tests/models/falcon/test_modeling_falcon.py",
          "additions": 2,
          "deletions": 2,
          "patch": "@@ -591,12 +591,12 @@ def test_batched_generation(self):\n \n         test_text = \"A sequence: 1, 2\"  # should generate the rest of the sequence\n \n-        unpadded_inputs = tokenizer([test_text], return_tensors=\"pt\").to(\"cuda:0\")\n+        unpadded_inputs = tokenizer([test_text], return_tensors=\"pt\").to(f\"{torch_device}:0\")\n         unpadded_gen_out = model.generate(**unpadded_inputs, max_new_tokens=20)\n         unpadded_gen_text = tokenizer.batch_decode(unpadded_gen_out, skip_special_tokens=True)\n \n         dummy_text = \"This is a longer text \" * 2  # forces left-padding on `test_text`\n-        padded_inputs = tokenizer([test_text, dummy_text], return_tensors=\"pt\", padding=True).to(\"cuda:0\")\n+        padded_inputs = tokenizer([test_text, dummy_text], return_tensors=\"pt\", padding=True).to(f\"{torch_device}:0\")\n         padded_gen_out = model.generate(**padded_inputs, max_new_tokens=20)\n         padded_gen_text = tokenizer.batch_decode(padded_gen_out, skip_special_tokens=True)\n "
        },
        {
          "file": "tests/peft_integration/test_peft_integration.py",
          "additions": 3,
          "deletions": 2,
          "patch": "@@ -35,6 +35,7 @@\n     require_bitsandbytes,\n     require_peft,\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_gpu,\n     slow,\n     torch_device,\n@@ -440,7 +441,7 @@ def test_peft_from_pretrained_kwargs(self):\n                 # dummy generation\n                 _ = peft_model.generate(input_ids=torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7]]).to(torch_device))\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     def test_peft_save_quantized(self):\n         \"\"\"\n@@ -479,7 +480,7 @@ def test_peft_save_quantized(self):\n                     self.assertTrue(\"pytorch_model.bin\" not in os.listdir(tmpdirname))\n                     self.assertTrue(\"model.safetensors\" not in os.listdir(tmpdirname))\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     def test_peft_save_quantized_regression(self):\n         \"\"\""
        }
      ]
    },
    {
      "sha": "effaef334bf21849f542c317542d8913217b3f6b",
      "message": "fix: prevent second save in the end of training if last step was saved already (#36219)\n\n* fix: prevent second save in the end of training\n\n* fix: prevent second save in the end of training\n\n* test: added test for no duplicate save on epoch save strategy\n\n* fix: removed TrainerControl\n\n* chore: style formatting\n\n---------\n\nCo-authored-by: JaktensTid <jaktenstid1@gmail.com>",
      "changes": [
        {
          "file": "src/transformers/trainer_callback.py",
          "additions": 1,
          "deletions": 1,
          "patch": "@@ -600,7 +600,7 @@ def on_step_end(self, args: TrainingArguments, state: TrainerState, control: Tra\n         if state.global_step >= state.max_steps:\n             control.should_training_stop = True\n             # Save the model at the end if we have a save strategy\n-            if args.save_strategy not in [SaveStrategy.NO, SaveStrategy.BEST]:\n+            if args.save_strategy == SaveStrategy.STEPS:\n                 control.should_save = True\n \n         return control"
        }
      ]
    }
  ]
}